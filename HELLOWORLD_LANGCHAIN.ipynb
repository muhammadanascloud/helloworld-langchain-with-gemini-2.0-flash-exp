{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKoKWjQzGRIQhmQwuv9ukm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammadanascloud/helloworld-langchain-with-gemini-2.0-flash-exp/blob/main/HELLOWORLD_LANGCHAIN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGDICObHQNAZ"
      },
      "outputs": [],
      "source": [
        "!python --version\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n"
      ],
      "metadata": {
        "id": "eY8c2310QZJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai\n"
      ],
      "metadata": {
        "id": "2wg6pr6bQqV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain\n"
      ],
      "metadata": {
        "id": "wjTT1gG3SAvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n"
      ],
      "metadata": {
        "id": "r1nJ3tekSBR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-experimental\n"
      ],
      "metadata": {
        "id": "b1DbnlDuTMvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q google-genai"
      ],
      "metadata": {
        "id": "5UA5_BneVKHV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Secret key ko access karo\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# API key ko environment variable me set karo\n",
        "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "print(\"API key loaded successfully from secrets!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVA0tamJVvp5",
        "outputId": "62b38670-3f99-4fc3-a8b5-f5f4ad9b8f1e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key loaded successfully from secrets!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Ensure API key is set in the environment\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    raise ValueError(\"API key not found! Please check your secrets.\")\n",
        "\n",
        "# Gemini model initialize karo\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-exp\",  # Specific model ka name\n",
        "    temperature=0.4  # Creativity level\n",
        ")\n",
        "\n",
        "print(\"Gemini 2.0 Flash Experimental model initialized successfully!\")\n"
      ],
      "metadata": {
        "id": "b4Fy9FyOW0o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prompt bhejo\n",
        "response = llm.invoke(\"Tell something interesting about Gemini 2.0 Flash model.\")\n",
        "print(\"Response:\", response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJYrnVBYXaxj",
        "outputId": "46f8925a-b11e-4d9d-9b09-bb3166cd7e23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response: Okay, here's something interesting about the Gemini 2.0 Flash model:\n",
            "\n",
            "**It's designed for speed and efficiency, but it's not just a \"lite\" version of Gemini.**\n",
            "\n",
            "While it's true that Flash is optimized for faster response times and lower resource consumption, it's not simply a stripped-down version of the larger Gemini models. Instead, it's been specifically engineered with a different architecture and training approach to excel at specific tasks.\n",
            "\n",
            "Here's why that's interesting:\n",
            "\n",
            "* **Targeted Optimization:** Instead of trying to be a jack-of-all-trades like the larger Gemini models, Flash is focused on being a master of certain domains. This means it's been trained and optimized for tasks where speed is paramount, such as:\n",
            "    * **Real-time text generation:** Think chatbots, quick summaries, and generating text for interactive applications.\n",
            "    * **Data extraction:** Quickly pulling key information from large datasets.\n",
            "    * **Code generation:** Generating code snippets efficiently.\n",
            "* **Strategic Trade-offs:** To achieve this speed, Flash makes strategic trade-offs. It might have a smaller parameter count or a different attention mechanism compared to the larger models. This means it might not be as capable at complex reasoning or nuanced understanding as its larger counterparts, but it's significantly faster and more efficient for its intended use cases.\n",
            "* **Complementary, not Competitive:** This makes Flash a complementary model to the other Gemini models, not a competitor. You wouldn't use Flash for tasks requiring deep analysis or creative writing, but you would use it for situations where speed and efficiency are crucial.\n",
            "* **Enabling New Applications:** The existence of Flash opens up possibilities for using AI in real-time applications that were previously too resource-intensive. This could lead to more responsive chatbots, faster data processing, and more seamless interactive experiences.\n",
            "\n",
            "**In essence, Gemini 2.0 Flash represents a shift in focus from \"bigger is better\" to \"right-sized for the task.\" It's a testament to the fact that AI models can be tailored to specific needs, leading to more efficient and practical applications.**\n",
            "\n",
            "This is interesting because it highlights a growing trend in AI development: moving beyond simply creating the largest and most powerful models to creating models that are optimized for specific use cases. This approach is crucial for making AI more accessible and practical for a wider range of applications.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C6HlM0auXbLT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}